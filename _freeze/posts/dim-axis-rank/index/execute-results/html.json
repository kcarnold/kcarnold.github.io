{
  "hash": "62061db91c9c0f8d4aa64a9eafbc2a44",
  "result": {
    "markdown": "---\ntitle: \"Multidimensional Array Terminology\"\ndate: \"2023-01-27\"\nkeywords: [\"ml\", \"coding\"]\n---\n\nA surprisingly large amount of the thinking that goes into implementing neural net code is getting the shapes of multidimensional structures right. I'd heard that from others but didnâ€™t really believe it until I had to figure it out myself a couple of times, and that convinced me that everyone could use some guided practice with that. So I give my AI students some exercises in thinking about the shapes of multidimensional structures. We're working with images because they're easier to visualize, but the same thing comes up in sequence modeling (batch by sequence length by embedding dimension, sometimes with an attention head dim in there too!).\n\nStudents start to explore what broadcasting does (before officially learning how it works), which lets you do cool things like inverting an image by just computing `1 - image`.\n\n## Axis or Dimension?\n\nProblem: both the [PyTorch](https://pytorch.org/docs/stable/notes/broadcasting.html) and [NumPy](https://numpy.org/doc/stable/user/basics.broadcasting.html) broadcasting docs tend to use the term \"dimension\". This is confusing because, e.g., [0.5, 0.25, 0.75] is a vector in 3d space but has just one axis.\n\nI'm guilty of sloppy use of this terminology too, but I suggest we use \"number of axes\" to refer to `len(some_array.shape)`. This aligns with the NumPy [Glossary](https://numpy.org/doc/stable/glossary.html#term-axis) and the `axis=` keyword common in NumPy functions. Unfortunately we'll need to remember that sometimes PyTorch uses \"dimension\", e.g,. 'array.ndim' and the `dim=` keyword argument (kwarg) to reduction methods like `softmax`.\n\n## Rank?\n\nTo make matters worse, the fast.ai book uses \"rank\" to refer to the number of axes of a tensor. But \"rank\" means something different in linear algebra. For example, a length-5 column vector times a length-4 row vector would give a matrix (tensor) with two axes (2-dimensional), with shape `(5, 4)`\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nt1 = torch.ones((5, 1)); t1\nt2 = torch.ones((1, 4)); t2\nt3 = t1 @ t2; t3\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nt3.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\ntorch.Size([5, 4])\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nt3.ndim\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n2\n```\n:::\n:::\n\n\nBut because of how we made it, we know that the matrix has [rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)) 1 in the linear algebra sense, as Torch confirms. (Note that the Torch computation is sensitive to numerical precision issues, though it works correctly in this case.)\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntorch.linalg.matrix_rank(t3)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\ntensor(1)\n```\n:::\n:::\n\n\n## The Exercise\n\nHere's the exercise, for anyone interested:\n\n> [Image Operations](https://cs.calvin.edu/courses/cs/344/23sp/fundamentals/u02n2-image-ops.ipynb) (show [preview](https://cs.calvin.edu/courses/cs/344/23sp/fundamentals/u02n2-image-ops.html), open in [Colab](https://colab.research.google.com/github/kcarnold/cs344/blob/main/static/fundamentals/u02n2-image-ops.ipynb))\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}