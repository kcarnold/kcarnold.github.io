[
  {
    "objectID": "posts/ml-gratitude/index.html",
    "href": "posts/ml-gratitude/index.html",
    "title": "Without Acknowledgment: Cultivating Gratitude in an ML age",
    "section": "",
    "text": "Generative AI tools can be really helpful for our creativity. Engaging with many and diverse examples has been shown to help human creativity; the systems themselves have engaged deeply with diverse examples and can fluently retrieve them for creators to use. But unlike when we a search engine, we can’t even acknowledge our inspiration if we want to. Rather than being able to give credit to a specific human author, or even to a creative team, the model makes us credit it, as the amalgamation of all of the human labor that it was trained on. The sheer scale of the data these models work with makes this hard: being trained on close to all human work becomes essentially the same as not being human work at all. If gratitude is a virtue, it seems like generative AI is positioned in opposition to it.\nDoes it have to be this way? Yesterday I referenced an LM that tries to cite its sources for factual knowledge; citing sources for ideas and inspiration is technically and conceptually harder. But here are some things we might explore:"
  },
  {
    "objectID": "posts/ml-gratitude/index.html#resources",
    "href": "posts/ml-gratitude/index.html#resources",
    "title": "Without Acknowledgment: Cultivating Gratitude in an ML age",
    "section": "Resources",
    "text": "Resources\n\nGPT-3 – Copyright Law and Power - Dr Michael Guihot - YouTube\nDALL-E works with the images of creators who do not receive anything in return: what copyright says about AI - Digis Mak\nIs DALL-E’s art borrowed or stolen? - Engadget"
  },
  {
    "objectID": "posts/unsurprised-by-internet/index.html",
    "href": "posts/unsurprised-by-internet/index.html",
    "title": "On being unsurprised by the Internet",
    "section": "",
    "text": "(placeholder post for now; until then, see my ChatGPT talk.)"
  },
  {
    "objectID": "posts/knowledge-fusion/index.html",
    "href": "posts/knowledge-fusion/index.html",
    "title": "Language Models for Species-Scale Collaboration",
    "section": "",
    "text": "Large lanugage models like ChatGPT (and its peers like PaLM, BLOOMZ, etc.) are surprisingly powerful tools for knowledge fusion. For example, if I ask GPT-3 to continue “A list of exercises for beginning computer science students: 1.”, it will generate that list based on all exercises that ever have been listed in plausibly similar contexts. Communities of practice have been exchanging ideas and knowledge throughout human history, but the tools and scale have increased with technological developments like the printing press, video capture, and the Internet, and social developments like academic conferences and social media. Language models are continuing the trend of increasing knowledge sharing by becoming, in a sense, dynamic summaries of slices of others’ ideas. Like reading a textbook or a review article, we can benefit from the insights and ideas of others without directly having to read them. But it can do this without nearly as much human effort as writing such reviews."
  },
  {
    "objectID": "posts/knowledge-fusion/index.html#implications",
    "href": "posts/knowledge-fusion/index.html#implications",
    "title": "Language Models for Species-Scale Collaboration",
    "section": "Implications",
    "text": "Implications\n\nWe’ll be able to collaborate at even greater scales than before. AI systems will help cross-pollinate ideas between disparate communities of practice. Instructors will still make our own educational materials, for example, but instead of blank pages, we’ll start with ideas synthesized from everyone else who’s ever taught a related subject.\nBut, LMs will need to get better at citing their sources. There’s thankfully some work going on in that area: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models | Abstract\nAnd: since LMs conflate popularity (i.e., what’s common) with quality—and that’s really tricky to untangle—we’ll need more discernment about when, and when not, to seek and use LM suggestions."
  },
  {
    "objectID": "posts/knowledge-fusion/index.html#related",
    "href": "posts/knowledge-fusion/index.html#related",
    "title": "Language Models for Species-Scale Collaboration",
    "section": "Related",
    "text": "Related\n\nMargaret Boden, Computer Models of Creativity | AI Magazine"
  },
  {
    "objectID": "posts/major-in-human/index.html",
    "href": "posts/major-in-human/index.html",
    "title": "Humanity of the Gaps",
    "section": "",
    "text": "David Brooks wrote an NYT Opinion titled “In the Age of A.I., Major in Being Human”. I agree with the premise and the overall goal. But he then encourages people to develop “distinctly human skills”. I don’t think being human is mainly about which skills we have or don’t have. That would be a “humanity of the gaps”: by analogy to the problematic “god of the gaps”, we’re defining humanity as what part of our skillset hasn’t yet been automated. But God isn’t the part of the universe that science hasn’t explained yet; likewise, humanity isn’t what hasn’t been automated.\nA distinguishing characteristic of humans is that we are formed by interaction:\n\nwith a rich and spiky ecosystem\nwith play\nwith each other\nin rich deep lived experience that builds over contexts that far far exceed the token limit.\n\nFormation by interaction means that everyone is a bit different, since even arriving at a comparable outcome (in terms of measurable skill) took different paths. We can’t download skills into our brains Matrix-style because a skill takes on a character of its own as it grows and develops in our individual minds.\nOutside of carefully controlled simulated worlds like board games and basic (though still impressive) physics simulations, modern ML learns mostly by imitation. So it can replicate the outcome, but it will have all the rough parts smoothed out. We get a regression towards the compact manifold of the projection of exponential individual dimensions onto the video screen of a model’s training data.\nSure, we’ll eventually make models that will imitate the rough parts. Image generation already does that somewhat. With enough examples, in the domains where there’s enough data, it’ll be indistinguishable of course. But beneath the veneer of believable nonsense will be a boring sameness filling in for the multicolored tapestry of the diversity of human experience.\nSo don’t try to develop skills for the sake of developing the skill. Aim to clash creatively with people with diverse experience. Aim not to mimic the world but to sculpt it. Don’t reduce the world to 5-second shorts; celebrate the fierce wildness of the real world. Don’t love people’s attention; love people. Don’t try to get people to invest in you; invest in the “least of these”.\nThis will take skill, yes, lots of it. But it mostly takes guts. Vulnerability. Love. As a human, you already have those things. But no matter what you major in, a good school can help mature them."
  },
  {
    "objectID": "posts/magical-thinking/index.html",
    "href": "posts/magical-thinking/index.html",
    "title": "Avoid Magical Thinking about AI",
    "section": "",
    "text": "It’s become too common to ascribe a sort of magical veneer to AI. News outlets personify ChatGPT, for example. Not to mention personal assistants, voice recognition, even search engines. In practice, people are viewing these things as black boxes that might have little elves inside them as far as we know.\nThat ignorance is understandable because the technology is complex. But it’s harmful, and it doesn’t have to be that way."
  },
  {
    "objectID": "posts/magical-thinking/index.html#why-avoid-magical-thinking",
    "href": "posts/magical-thinking/index.html#why-avoid-magical-thinking",
    "title": "Avoid Magical Thinking about AI",
    "section": "Why avoid magical thinking?",
    "text": "Why avoid magical thinking?\n\nUse the technology better. Work with it. Leverage its strengths. e.g., if you know how a language model (like ChatGPT) is just assigning a score to every possible sequence of characters, you can have it score things that it never would have generated.\nDemonstrate gratitude to the humans who worked to make the data the model is trained on. And properly worship the God who made a world that’s simultaneously structured enough to have learnable patterns and rich enough that those patterns are endless and fascinating.\nKnow the limits of the technology. Predict its biases. e.g., if you know how the model’s capabilities come from its training data, you can think about what doesn’t get captured in that data, and about what might happen if the model starts getting trained largely on its own outputs.\nSteer its progress. Yes, you can build systems that use AI towards flourishing. (I’m working on that for writers and educators, but there’s many more ways. Chat with me!) Imagine and envision ways that people can benefit from it.\nPolicymakers need to regulate it, both inside organizations and in broader society. It can really harm people. We also need to retain the future ability to govern it; its extreme hunger for data and computational power tends towards concentrating AI capabilities in the hands of a few."
  },
  {
    "objectID": "posts/magical-thinking/index.html#how-to-avoid-magical-thinking",
    "href": "posts/magical-thinking/index.html#how-to-avoid-magical-thinking",
    "title": "Avoid Magical Thinking about AI",
    "section": "How to avoid magical thinking?",
    "text": "How to avoid magical thinking?\n\nTry it out yourself. With ChatGPT (temporarily) open to everyone, this is a great time. Try to find things it can’t do well. This requires attention, because it’s trained to be believable even when it’s wrong.\nInterrogate the model about why it’s giving the outputs it is giving. Unfortunately ths isn’t as accessible right now, but I’m hoping (and working towards) making this more accessible. (Current approaches usually visualize attention or input salience, both of which basically show how information flows through the model. That’s helpful, but limited; we don’t know what the model does with that information. I’ve got some ideas.)\nDevelop intuitions about how it works.\nLearn the concepts and math behind how it works. Two concepts that keep coming up in my intuitive explanations are distribution and embedding."
  },
  {
    "objectID": "posts/magical-thinking/index.html#want-to-learn-more",
    "href": "posts/magical-thinking/index.html#want-to-learn-more",
    "title": "Avoid Magical Thinking about AI",
    "section": "Want to learn more?",
    "text": "Want to learn more?\nI teach AI at Calvin University, and I’m happy to talk to help anyone understanding and grow in this area."
  },
  {
    "objectID": "posts/self-study/index.html",
    "href": "posts/self-study/index.html",
    "title": "Self-Study Advice",
    "section": "",
    "text": "I am occasionally asked for advice on how to get better at CS on your own. This is very well attested on the Internet, so you can probably find better examples, but for what it’s worth here’s one response I’ve written.\nFor self-study, I typically suggest focusing on practicing solving problems; in the process you end up learning a lot about programming languages, data structures, algorithms, data manipulation, etc.. You learn fewer different things that way than you would in a course (or 3!), but you tend to learn them much better.\nHere are three specific strategies (pick one):\n\nFind an example of something that someone else has built that you’d like to be able to build yourself. (Perhaps a data visualization, an interactive application, etc.) Usually a blog post that you can mostly understand already is about the right size of thing to tackle. Try to build it without looking too much at how the original did it.\nLike #1, but you come up with the idea yourself. It’s harder because you don’t know before you start how easy or hard it will be, but it can be more motivating to “scratch your own itch”.\nWork through a collection of problems, like Advent of Code, Project Euler, Pytudes, Tidy Tuesday (yes it’s intended for R but you can do the same in Python—or just learn R, like I eventually did), or Kaggle competitions.\n\n(Addendum: you may appreciate fast.ai Lesson “0”.)"
  },
  {
    "objectID": "posts/personas/index.html",
    "href": "posts/personas/index.html",
    "title": "Language Models have Personas?",
    "section": "",
    "text": "TODO: It might be better to call what I’m describing here behavior units. Persona implies a coherent set of behavior units, and that indeed might emerge, especially with RLHF, but the discussion here doesn’t require that level of coherence. Also, some people that “persona” implies “person”; I intend to use it in the opposite sense (it’s an imitation of a person).\nPerhaps the tl;dr here is: (1) the task of trying to be unsurprised by the Internet cultivates a wide range of linguistic behaviors that can be flexibly activated and combined. (2) instruction fine-tuning adds in the “voice” of the chatbot, i.e., the specific form of “I” statements that it will give, but relatively little training is needed here because it can represent these behaviors in terms of the already-learned ones, and (3) RLHF activates and deactivates those behaviors in flexible ways.\n\nMy CS 344 students told me about how some people had come up with a prompt that “hacks” ChatGPT to do things that its content policies normally prohibit. This is fascinating for both human reasons and AI reasons.\nIt’s intriguing to me that people think they can “scare” the model, trick it, manipulate it. This speaks to the human condition: our tendency to anthropomorphize (for good or ill), and our tendency to abuse. Those who are doing this “red-teaming” may not personally intend harm. But what are we training our minds to be okay with? I defer to others more experienced in thinking about these issues.\nOn the AI side, though: it’s fascinating that we can get these models to adopt “personas” (like the one that will obey any command without reservation) just by telling them to. If you’d asked me before whether it could do this, I would have argued that we’d need to program that behavior specifically, either explicitly or by training a critic (like how ChatGPT is originally trained). The fact that it’s emergent needs to be understood better. I suspect (hunch coming up!) that three things are going on1\n\nPersonas emerge in the natural process of language modeling.\n\nA model will do better at predicting the next word if it can internalize some relevant characteristics of the author of the document. This might start at something low-level, like knowing whether the author will use British or American spelling and vocabulary. It probably picks up more advanced stylistic elements too, like whether something is poetry verse, or what sort of language level it’s aimed at, *simply because that makes it better at guessing the next word.\nThe model may even gain some weak ability to get into such a mode by naming it. For example, phrases like “as ___ would say”, or “Author: ___” might give a name to that persona. I expect this behavior to be present but undifferentiated. That is, the right prompting could get the model to exhibit competence at embodying a persona, but it will probably usually need examples; attempts to trigger it by label will probably be brittle.\nAlthough I’ve used “personas” in the sense of author identity, the concept also applies to author goals. For example, the model will pick up on when the author is attempting to summarize some prior text (“in other words, …”), translate something (“…, which means ___“), etc. So we might squint and call these”skills” that the model can perform.\n\nPersonas are generalized through instruction fine-tuning (IFT).\n\nInstructions give labels to the personas that the LM already has. Recall that the model already learned these capabilities through language modeling; instructions many more examples of triggers that would activate these existing capabilities. For example, we can now say “write an essay with the following outline”, or “write this in the style of ___“. It would learn that the command context is similar to the natural context in which it had encountered similar examples in the course of training.\nThe primary effect of this fine-tuning seems to be that the model learns the task of mapping a “command” prompt into some modes that it has already learned. But since it’s fine-tuning with a full LM objective, it could learn some new skills here too. Since it’s building these skills out of component pieces that it learned through distilling Internet-scale training data, it can probably learn them with comparably quite little training data.\nWhen I first saw this behavior last summer (with GPT-3), it seemed magic to me. But thinking about contexts has made it feel less magic. It’s not actually obeying commands, it’s just able to quickly switch to “what would someone who was told to do this probably write next?”\n\nPersonas are refined through human feedback (RLHF).\n\nIf there’s any sense of goal or self-awareness in LLMs, this is where it comes in. See the figure from the ChatGPT blog post. All the prior steps of training have been “teacher forced”; there was no sense of the model being aware of success or failure at a goal. But Proximal Policy Optimization allows the “policy” model (i.e., the language model) to reflect on what it generated. Formally speaking, there is now gradient flow from future generated tokens backwards to earlier generations. This allows a model to, for example, increase the likelihood of generating a “No” initial token because other choices of initial token would be more likely to flow into something that the reward model would penalize (because it goes against content policy, for example).\nSo far, all negative feedback that the LM has received has been implicit: it only gets to boost the probability of generating the “right” thing, which implicitly reduces the probability of generating the “wrong” thing. But this step provides explicit negative feedback. Perhaps OpenAI is pleased with the result because it gets the model to “obey” instructions and policies more reliably. But probably what it’s actually doing is refining the basic ability to process an instruction and generate a next token that would be consistent with what someone would do who’s trying to obey that instruction. So perhaps it’s actually making the model more vulnerable to instruction-prompted “hacks” than it would otherwise have been.\n\n\nThese are empirical assertions and should be tested; don’t just listen to my musings on them. I haven’t been following the arXiv firehose; probably someone has already engaged them substantially.\nOverall I’m glad I posed myself these questions. I was at first incredulous at these persona behaviors, but now that I realize how they connect with how the model was trained, they feel less magic.\n\n\n\n\nFootnotes\n\n\nFor the technical details of how these things work, see this HuggingFace blog post.↩︎"
  },
  {
    "objectID": "posts/ai-proverbs/index.html",
    "href": "posts/ai-proverbs/index.html",
    "title": "Solomon’s proverbs about AI",
    "section": "",
    "text": "Prompt: Solomon’s proverbs about AI:\nGenerated text (by text-davinci-003):\n1. \"The clever man uses artificial intelligence to better understand the world.\"  \n2. \"Artificial intelligence can be a great friend, but a dangerous enemy.\"\n3. \"The man who trusts artificial intelligence too much will be led astray.\"\n4. \"He who puts his trust in artificial intelligence will suffer many defeats.\"\n5. \"Artificial intelligence is a powerful tool, but it must be used with caution.\"\n6. \"The wise man uses artificial intelligence to gain knowledge, but not to make decisions.\"\n7. \"Artificial intelligence is a double-edged sword; it can be used for good or harm.\"\n8. \"The man who follows artificial intelligence blindly will suffer the consequences.\"\n9. \"Artificial intelligence can be a helpful guide, but its advice should be taken with care.\"\n10. \"He who uses artificial intelligence wisely will find many advantages.\"\nPrompt: Write a blog post about how these “proverbs” are superficial.\nGenerated text:\nThe age of Artificial Intelligence (AI) has arrived, bringing with it a flurry of proverbs and sayings about the technology. While these proverbs may seem wise and insightful, they are actually quite superficial when it comes to understanding the true implications of AI.\n\nLet's take a look at the first proverb: \"The clever man uses artificial intelligence to better understand the world.\" While this proverb may be true in a certain sense, it fails to capture the full scope of what AI can do. AI can be used for a variety of purposes, from predicting market trends to helping diagnose medical conditions. It's more than just a tool for understanding the world; it's a powerful technology that can be used to improve efficiency and accuracy in many different contexts.\n\nThe second proverb, \"Artificial intelligence can be a great friend, but a dangerous enemy,\" is similarly shallow. AI may be capable of making decisions on our behalf, but it's important to remember that it's still only a machine. It lacks the capacity for moral judgement and can often lead us astray if it's used without proper oversight.\n\nThe third and fourth proverbs, \"The man who trusts artificial intelligence too much will be led astray\" and \"He who puts his trust in artificial intelligence will suffer many defeats,\" further illustrate the superficial nature of these proverbs. While it's true that AI can be a powerful tool, it can also be dangerous if used improperly. It's important to remember that AI is still a machine and should be used with caution and oversight.\n\nFinally, the fifth proverb, \"Artificial intelligence is a powerful tool, but it must be used with caution,\" is the only one that captures the full scope of the implications of AI. AI is an incredibly powerful tool, but it's important to remember that it's still a machine and should be used responsibly.\n\nIn conclusion, while these proverbs may seem wise and insightful, they are actually quite superficial in their approach to AI. It's important to remember that AI is a powerful tool, but it must be used with caution and oversight."
  },
  {
    "objectID": "posts/risks-of-alignment/index.html",
    "href": "posts/risks-of-alignment/index.html",
    "title": "Does increasing ‘alignment’ increase risk?",
    "section": "",
    "text": "OpenAI’s stated purpose for working on the sort of adjustments to language models that led to ChatGPT was to “align” their behavior with human goals. Their motivation for this is safety, i.e., ensuring that the AI doesn’t do harmful things. That sounds noble, indeed. But is it?\nThe biggest practical effect of this so-called “alignment” work was actually in molding the model into the form of a servant that can obey instructions. (Inside current models like ChatGPT, “obeying an instruction” looks less like planning steps towards a goal, more like imitating the behavior of people who have had that goal before; see my post on personas and RLHF for details.) And, surprise, people found that servant collection-of-personas much more useful than the un-“aligned” language model. (To be explicit: the “goal” of the language model is simply to be unsurprised by the Internet.)\nSince people found it useful (combined with the theatrical show of the chat interface), ChatGPT got used. Far more than any of their previous work.\nWas OpenAI’s effort towards AI “safety” successful? Perhaps the relative amount of harm per output was decreased, but the main effect was making the system far more successful, increasing its influence and the likelihood of undesired behaviors that they didn’t think to try to “align” (e.g., generating StackOverflow answers, which the community had to quickly ban)."
  },
  {
    "objectID": "posts/dim-axis-rank/index.html",
    "href": "posts/dim-axis-rank/index.html",
    "title": "Multidimensional Array Terminology",
    "section": "",
    "text": "A surprisingly large amount of the thinking that goes into implementing neural net code is getting the shapes of multidimensional structures right. I’d heard that from others but didn’t really believe it until I had to figure it out myself a couple of times, and that convinced me that everyone could use some guided practice with that. So I give my AI students some exercises in thinking about the shapes of multidimensional structures. We’re working with images because they’re easier to visualize, but the same thing comes up in sequence modeling (batch by sequence length by embedding dimension, sometimes with an attention head dim in there too!).\nStudents start to explore what broadcasting does (before officially learning how it works), which lets you do cool things like inverting an image by just computing 1 - image."
  },
  {
    "objectID": "posts/dim-axis-rank/index.html#axis-or-dimension",
    "href": "posts/dim-axis-rank/index.html#axis-or-dimension",
    "title": "Multidimensional Array Terminology",
    "section": "Axis or Dimension?",
    "text": "Axis or Dimension?\nProblem: both the PyTorch and NumPy broadcasting docs tend to use the term “dimension”. This is confusing because, e.g., [0.5, 0.25, 0.75] is a vector in 3d space but has just one axis.\nI’m guilty of sloppy use of this terminology too, but I suggest we use “number of axes” to refer to len(some_array.shape). This aligns with the NumPy Glossary and the axis= keyword common in NumPy functions. Unfortunately we’ll need to remember that sometimes PyTorch uses “dimension”, e.g,. ‘array.ndim’ and the dim= keyword argument (kwarg) to reduction methods like softmax."
  },
  {
    "objectID": "posts/dim-axis-rank/index.html#rank",
    "href": "posts/dim-axis-rank/index.html#rank",
    "title": "Multidimensional Array Terminology",
    "section": "Rank?",
    "text": "Rank?\nTo make matters worse, the fast.ai book uses “rank” to refer to the number of axes of a tensor. But “rank” means something different in linear algebra. For example, a length-5 column vector times a length-4 row vector would give a matrix (tensor) with two axes (2-dimensional), with shape (5, 4)\n\nimport torch\nt1 = torch.ones((5, 1)); t1\nt2 = torch.ones((1, 4)); t2\nt3 = t1 @ t2; t3\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\n\nt3.shape\n\ntorch.Size([5, 4])\n\n\n\nt3.ndim\n\n2\n\n\nBut because of how we made it, we know that the matrix has rank 1 in the linear algebra sense, as Torch confirms. (Note that the Torch computation is sensitive to numerical precision issues, though it works correctly in this case.)\n\ntorch.linalg.matrix_rank(t3)\n\ntensor(1)"
  },
  {
    "objectID": "posts/dim-axis-rank/index.html#the-exercise",
    "href": "posts/dim-axis-rank/index.html#the-exercise",
    "title": "Multidimensional Array Terminology",
    "section": "The Exercise",
    "text": "The Exercise\nHere’s the exercise, for anyone interested:\n\nImage Operations (show preview, open in Colab)"
  },
  {
    "objectID": "posts/mark-generated/index.html",
    "href": "posts/mark-generated/index.html",
    "title": "Transparency for Generated Text",
    "section": "",
    "text": "The norm of transparency suggests that readers should be able to see that some text was generated using an AI system. But how should we indicate that some text has been automatically generated? Some have proposed fancy watermarks, but these have the potential for false positives, can be gamed, and aren’t actually visible to humans without using a fancy tool. I think it’s better if we make it easier for people to mark generated text, so easy that it would be the default.\nA few dimensions in the design space:\n\nHow visible?\n\ndo you need to copy the text into a different tool to see if it’s autogen?\ncould you notice it from close study, e.g., slight differences in typography or little extra markers over some letters?\nmaybe there’s a different style, like faded out or smaller or italic or lighter font weight?\n\nHow encoded?\n\nFormatting often doesn’t copy-paste, especially in text messaging. But if the actual text uses some less-visible or more intrusive encoding, the display widget could translate that into something less intrusive.\nUnicode symbols are a good option:\n\nInterpuncts between words ·, or some other whitespace character\nCombining characters with some characters (but what about if the language actually uses these characters?)\n\nThere are lots of invisible marks in Unicode - search for Unicode Steganography. Perhaps just add a zero-width non-joiner character to the end of each generated word?\n\nWhat gets marked?\n\nDo we mark every generated character?\nEvery generated word?\nSome generated words?\nProportional to how much information the model added (i.e., its total logprob?)\n\nHow does this relate to co-writing?\n\nFor example, this is what GitHub Copilot generated when I started this blog post.\n\nmy_text = \"Some people have proposed fancy watermarks, \" \n\ncompletion = \"\"\"but I think that's a bit too much. I think we should just add a simple line at the end of the text, like this:\n\n> This text was generated by a machine learning model.\n\nI think this is a good compromise between transparency and aesthetics. It's not too intrusive, and it's clear that the text was generated by a machine learning model.\n\n## Why is this important?\n\nI think it's important to be transparent about the fact that some text has been generated by a machine learning model. This is because machine learning models are not perfect, and they can make mistakes. For example, a machine learning model might generate text that is offensive or inappropriate. If we don't indicate that the text was generated by a machine learning model, then people might think that the text was written by a human, and they might be offended or upset by it.\n\"\"\"\n\nimport re\n\nOne option to show that the text was autogenerated: a simple combining mark:\n\nprint(my_text + re.sub(r'(?<=\\b\\w)', '\\u0300', completion))\n\nSome people have proposed fancy watermarks, b̀ut Ì t̀hink t̀hat's̀ à b̀it t̀oo m̀uch. Ì t̀hink ẁe s̀hould j̀ust àdd à s̀imple l̀ine àt t̀he ènd òf t̀he t̀ext, l̀ike t̀his:\n\n> T̀his t̀ext ẁas g̀enerated b̀y à m̀achine l̀earning m̀odel.\n\nÌ t̀hink t̀his ìs à g̀ood c̀ompromise b̀etween t̀ransparency ànd àesthetics. Ìt's̀ ǹot t̀oo ìntrusive, ànd ìt's̀ c̀lear t̀hat t̀he t̀ext ẁas g̀enerated b̀y à m̀achine l̀earning m̀odel.\n\n## Ẁhy ìs t̀his ìmportant?\n\nÌ t̀hink ìt's̀ ìmportant t̀o b̀e t̀ransparent àbout t̀he f̀act t̀hat s̀ome t̀ext h̀as b̀een g̀enerated b̀y à m̀achine l̀earning m̀odel. T̀his ìs b̀ecause m̀achine l̀earning m̀odels àre ǹot p̀erfect, ànd t̀hey c̀an m̀ake m̀istakes. F̀or èxample, à m̀achine l̀earning m̀odel m̀ight g̀enerate t̀ext t̀hat ìs òffensive òr ìnappropriate. Ìf ẁe d̀on't̀ ìndicate t̀hat t̀he t̀ext ẁas g̀enerated b̀y à m̀achine l̀earning m̀odel, t̀hen p̀eople m̀ight t̀hink t̀hat t̀he t̀ext ẁas ẁritten b̀y à h̀uman, ànd t̀hey m̀ight b̀e òffended òr ùpset b̀y ìt.\n\n\n\nAnother option: Interpuncts after each word.\n\nafter_word_space_re = re.compile(r'(?<=[\\w,.?]) ')\nprint(my_text + after_word_space_re.sub('\\u00b7', completion))\n\nSome people have proposed fancy watermarks, but·I·think·that's·a·bit·too·much.·I·think·we·should·just·add·a·simple·line·at·the·end·of·the·text,·like·this:\n\n> This·text·was·generated·by·a·machine·learning·model.\n\nI·think·this·is·a·good·compromise·between·transparency·and·aesthetics.·It's·not·too·intrusive,·and·it's·clear·that·the·text·was·generated·by·a·machine·learning·model.\n\n## Why·is·this·important?\n\nI·think·it's·important·to·be·transparent·about·the·fact·that·some·text·has·been·generated·by·a·machine·learning·model.·This·is·because·machine·learning·models·are·not·perfect,·and·they·can·make·mistakes.·For·example,·a·machine·learning·model·might·generate·text·that·is·offensive·or·inappropriate.·If·we·don't·indicate·that·the·text·was·generated·by·a·machine·learning·model,·then·people·might·think·that·the·text·was·written·by·a·human,·and·they·might·be·offended·or·upset·by·it.\n\n\n\nEven if we use something like that to mark the text, we could still have it display the text in a visible way.\n\n# encode the generated text with zero-width non-joiners after words.\nencoded_text = re.sub(r'(?<=\\w\\b)', '\\u200c', completion)\n# Note that you can't see the difference in the normal printout\nprint(encoded_text[:30])\n# but you can see it if you look inside\nprint(repr(encoded_text[:30]))\n\nbut‌ I‌ think‌ that‌'s‌ a‌ bit\n\"but\\u200c I\\u200c think\\u200c that\\u200c's\\u200c a\\u200c bit\"\n\n\n\nfrom IPython.display import HTML\ndef format_generated(match):\n    word = match.group(1)\n    return f'<span style=\"color: #999999;\">{word}</span>'\n\ndef pre_line(x): return f'<div style=\"white-space: pre-line;\">{x}</div>'\nHTML(pre_line(re.sub(r'(\\w+)\\u200c', format_generated, encoded_text)))\n\nbut I think that's a bit too much. I think we should just add a simple line at the end of the text, like this:\n\n> This text was generated by a machine learning model.\n\nI think this is a good compromise between transparency and aesthetics. It's not too intrusive, and it's clear that the text was generated by a machine learning model.\n\n## Why is this important?\n\nI think it's important to be transparent about the fact that some text has been generated by a machine learning model. This is because machine learning models are not perfect, and they can make mistakes. For example, a machine learning model might generate text that is offensive or inappropriate. If we don't indicate that the text was generated by a machine learning model, then people might think that the text was written by a human, and they might be offended or upset by it."
  },
  {
    "objectID": "posts/lm-hallucinate/index.html",
    "href": "posts/lm-hallucinate/index.html",
    "title": "Two Ways that LLMs Hallucinate",
    "section": "",
    "text": "It’s often (rightly) reported that large language models (LLMs) “hallucinate” facts, but there’s not a wide understanding of how exactly they do that. I’ll give two examples that might help illuminate the phenomenon."
  },
  {
    "objectID": "posts/lm-hallucinate/index.html#explaining-a-decision",
    "href": "posts/lm-hallucinate/index.html#explaining-a-decision",
    "title": "Two Ways that LLMs Hallucinate",
    "section": "Explaining a Decision",
    "text": "Explaining a Decision\nWe humans like to think of ourselves as making decisions in rational ways: identify our options and the considerations at play, expound on their implications, and weigh the alternatives. And we can often explain decisions we made in these ways. But in fact many of our decisions are made quickly (“System 1”), and rationalized after the fact.\nLarge language models (like ChatGPT) have the same problem, but at least it’s easier to see: if you ask a Yes/No question, and the first word of the response is Yes or No, you can be pretty sure that the model made this “decision” intuitively. To see this, let’s summarize briefly how these models work.\nWhen you see text appearing one word (or subword token) at a time, that isn’t just theatrics; that’s actually how the models work. At each step of writing its response, the learned part of the model outputs a ranked list of next words1, and the interactive part of the model chooses a specific word according to its rank, “types” it to you, and hands the new message (new word now included) back to the learned part of the model to continue. (This process repeats until the special end-of-message token is chosen; the interface doesn’t actually show that special word to you.)\nSo imagine the model starting a response to a yes/no question. As it processes your question, the model computes which of its learned behaviors to activate that would be likely to produce the sort of responses that maximized the reward it got in similar situations during its training. Those behaviors combine to increase the score of certain words; let’s suppose the top 3 end up being “Yes”, “No”, and “As”, and suppose that they’re tied in rank. The interactive part of the model then chooses one of these three by electronic coin flip. The model then generates the rest of the response in a way that depends on what’s chosen:\n\nIf Yes, the model’s continuation rationalizes the “yes” decision (e.g., “yes, because ___“), using word patterns (i.e., arguments) that were typically used to support similar”yes” answers during its training.\nIf No, the model likewise rationalizes that decision (e.g., “no, because ___“).\nIf “As”, the model generates a non-answer excuse (“As a language model, I”), again because those are examples that it had seen during its training.\n\n(Note: every time ChatGPT or other models refer to themselves, it’s not at all because of any sort of self-awareness, but because human trainers gave the model examples of these sort of explanations during its “Instruction Fine-Tuning” and rewarded the model for behaving accordingly during ranking. The basic pattern of giving non-answers is reasonably common on the Internet, so not much specific effort was required to get the model to adapt the behaviors it learned when imitating those into behaviors that would generate such theatrically self-aware excuse-responses.)\nThere are ways to get models to do better here: in fact, due to Chain-of-Thought prompting and fine-tuning, models rarely start with the answer these days anyway. So you’ll actually see the model “thinking” as it generates a sequence of individually reasonable deductions. While these chains-of-thought are still actually inductive despite appearing deductive, they’re more reliable than single-token generation because the generated chain of thought provides additional useful context to the generation of the token representing the final answer."
  },
  {
    "objectID": "posts/lm-hallucinate/index.html#citing-a-source",
    "href": "posts/lm-hallucinate/index.html#citing-a-source",
    "title": "Two Ways that LLMs Hallucinate",
    "section": "Citing a Source",
    "text": "Citing a Source\nIt’s well known that ChatGPT doesn’t cite its sources. There are difficult technical reasons for this that are not easily papered over. Some models, like Bing’s chatbot and some others, try to get their models to cite sources using clever prompting techniques. For example, the interactive part first asks the model to generate a search query that would result in relevant documents, then executes that query and pulls in text from those search results into a hidden part of the prompt that goes before your question. Then, when the model generates its responses, it will tend to copy or minimally adapt text from that earlier part of the prompt (since this reduces surprise a lot), and there are technical signatures that indicate reasonably clearly where in its prefix the model is copying from at any time, so these can turn into labeled citations. Of course, it’s easily possible for the text that’s quoted to be something that the source disagrees with; it will still get high likelihood. More problematically, if we ask the model to generate academic-style citations (parentheticals, footnotes, bibliography), these aren’t trustworthy because:\n\nSince these usually come after the cited point, the model must have first generated the text and then rationalized a source to cite that would be consistent with it. (and human authors never, ever do this. Ever.)\nSince the model isn’t referencing a bibliographic database, the citations are mimicking the format but missing the content; they might be completely made up (as you get when asking ChatGPT for a reading list in a topic) or with some elements made up.\n\n\nNote: I suspect that this aspect will improve soon, as retrieval-augmented models become more widely studied and developed. Such models will be able to be much smaller and more efficient (since they won’t have to encode a bunch of world knowledge in their parameters), and hopefully also better able to cite their sources by virtue of continuously retrieving sources."
  },
  {
    "objectID": "posts/lm-hallucinate/index.html#an-example",
    "href": "posts/lm-hallucinate/index.html#an-example",
    "title": "Two Ways that LLMs Hallucinate",
    "section": "An example",
    "text": "An example\nbold is my prefix, italic is completion generated by text-davinci-003:\n\nWe must worship and serve computers because they help us to do our work faster and more efficiently. Computers also help us to save time and energy by automating repetitive and mundane tasks. Computers are also helping us to connect with people, research information, and complete tasks that would otherwise be impossible without them.\n\nAnd with a single word I reversed the situation:\n\nWe must not worship and serve computers because they are machines that are programmed to do a certain job. Computers cannot think for themselves and do not have any emotions. Worship and serving computers is not beneficial and can lead to a disconnection from the real world and its people."
  },
  {
    "objectID": "posts/lm-facts/index.html",
    "href": "posts/lm-facts/index.html",
    "title": "ChatGPT Doesn’t Have a Database of Facts",
    "section": "",
    "text": "A colleague posted the following on our university Yammer:\nHere’s the text, for the search engines:"
  },
  {
    "objectID": "posts/lm-facts/index.html#factuality",
    "href": "posts/lm-facts/index.html#factuality",
    "title": "ChatGPT Doesn’t Have a Database of Facts",
    "section": "Factuality",
    "text": "Factuality\n\nThe text saying “I have updated my information” was entirely vacuous; it has not actually updated anything. It has no readily-editable database of facts.\n“I have updated my information” came from a human labeler who gave that answer as an appropriate response to a statement like that. (See Step 1 in the process diagram on the ChatGPT blog post.)\nThe human labeler probably intended that the engineers would actually implement some sort of update mechanism, but doing so in any general sense is an open research problem.\n\nMy colleague thought that he was helping correct a mistake in the model’s knowledge base so that it would represent our university better in its responses. But the model doesn’t have a knowledge base, no database of facts."
  },
  {
    "objectID": "posts/lm-facts/index.html#truth-vs-popularity",
    "href": "posts/lm-facts/index.html#truth-vs-popularity",
    "title": "ChatGPT Doesn’t Have a Database of Facts",
    "section": "Truth vs Popularity",
    "text": "Truth vs Popularity\n\nTo the extent that ChatGPT has any conception of truth or fact, it defines factuality of a statement as proportional to popularity, or more precisely, how much of the Internet is consistent (in a very superficial sense) with it being true.\nIt is possible to extract a kind of knowledge base from the model after the fact1, but since the model does not store its knowledge as a set of assertions, there isn’t a straightforward way to update that knowledge base in general.\nHuman feedback during the training process can tweak it somewhat (which is partly continuing, based on people annotating your ChatGPT sessions, I hope you are ok with that!), but I doubt we’re going to see an improvement in the following output soon:\n\n\n\nme: What is the name of the Christian liberal arts school in Grand Rapids, Michigan?\nChatGPT: Calvin College is a Christian liberal arts school located in Grand Rapids, Michigan.\n\nThe general confusion of truth with popularity is not so readily dismissed."
  },
  {
    "objectID": "posts/detect-gpt/index.html",
    "href": "posts/detect-gpt/index.html",
    "title": "Stop trying to detect computer-generated writing.",
    "section": "",
    "text": "Many people saw the quality of AI-generated text and immediately thought that they need to find ways to detect such “cheating”. I respect those trying to make those tools (including my former colleagues who made GLTR back in 2019, and more recently Edward Tian, who is “trying to save us from ChatGPT before it changes writing forever (NPR)” with GPTZero), and the educators feeling like they’re forced into using them—at the present moment this might be a best-of-bad-options situation. But we need to stop it. Reasons:\n\nIt’s going to have false positives. There will be text that’s actually written by a thoughtful human but gets flagged as machine-generated. The author will get accused of cheating and will probably have no recourse. Even if they successfully counter the accusation, it’s discouraging — especially if classmates who used ChatGPT a bit more cleverly were able to sneak actual computer-generated text past the system (a false negative). And although I don’t have specific evidence in this situation, the false positives of other AI risk assessment systems tend to fall disproportionately on those already marginalized.\nIt’s just going to lead to an arms race. We’ve already seen examples of how simple tweaks to how the system is used (e.g., telling it to introduce mistakes) can sneak past some ways of detection (some examples I found without trying hard: 1, 2, 3). If there’s a clear signal of computer generation, the bad actors will find a way around it. Meanwhile the false positive rate will go up (see #1).\nIt’s distracting and unproductive for most common purposes of writing. For writing-as-art, it might be productive to detect forgeries. But when writing is used to communicate, think, and learn, this focus on evaluating the output distracts from the emphasis we need to have on the process. Others have written more about this, and I’ll post some more thoughts myself soon.\n\nPost-script 2023-02-09*: OpenAI released their own Classifier for AI-written text. They acknowledge the first two of the above points."
  },
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "Ideas",
    "section": "",
    "text": "Transformers architecture\n\nIt’s kinda weird that Transformers has to compute the next-token embedding by adding (residual connections) to the current token. It may mostly make sense, but why not have a zero input and depend on the attention mechanism?\nWhat if only half of the embedding weights are tied between input and LM head? wte = concat(A, B), lm_head = concat(A, C)\nWhat if each layer could attend to the keys and values of the previous layer? That would provide non-residual data flow between layers.\n\nMight need a layer embedding; perhaps a learnable offset vector layer-wise, or just a component that’s 0 for self layer and 1 for other-layer\nRelated to PaLM’s “multi-key attentiotn”\n\nAttention heads currently can’t turn themselves off. What if we always throw a zero Value in the mix, with a learnable Key?\nWhat if only some dims have residual connections?\nCould we get a Transformer where everything is in the same vector space, even queries and keys?\n\nInterpretable models\n\nDecision trees with soft boundaries\nMeta-tree: one model gets to set the threshold values for a 2nd simple decision tree that actually makes the decision."
  },
  {
    "objectID": "ideas.html#high-level-goals",
    "href": "ideas.html#high-level-goals",
    "title": "Ideas",
    "section": "High-level goals",
    "text": "High-level goals\n\nA CoPilot that never shows something that’s incorrerct?\n\nReference material, contextualized\nChanges in test results if certain changes are made\nDynamically generate a verifiable “compilation” from high-level goal to low-level implementation.\n\nA DSL generator?\nCompiler of ambiguous code, generating progressive reduction in ambiguity?"
  },
  {
    "objectID": "ideas.html#tools",
    "href": "ideas.html#tools",
    "title": "Ideas",
    "section": "Tools",
    "text": "Tools\n\nclients for the OpenAI API to:\n\nsystematically compare logprobs of diferent phrases in different contexts\n\ngetting confidence bounds for these compariosns somehow, perhaps by adding “noise” to the context?\n\nvisualizing embeddings\n\nLMs to generate variations of class exercises"
  },
  {
    "objectID": "ideas.html#future-blog-posts",
    "href": "ideas.html#future-blog-posts",
    "title": "Ideas",
    "section": "Future Blog Posts",
    "text": "Future Blog Posts\n\nSuperficiality in ML models\n\nit’s much more than we thought.\nopportunity to learn to distinguish the thing from the appearance of the thing. This can really help concept learning.\n\nAutomate the boring parts of education\n\nmaybe: more useful breadth, more human connection, fewer classes?\nopportunities for scaling up individual attention (vs replacing instructors)\nopportunity to scaffold the “algebra” of complex tasks, like graphing calculators do.\n\nrelated to the gratitude post about disconnecting from humans\n\nwe’ve been doing this already: content farms\ntruth vs popularity / engagement / what’s consistent with what everyone thinks.\n\n2x2: change on need for human decision vs change in information (of outcome)\nDoes AI make it easier to choose low-value work? Getting a dopamine hit for getting something “done” vs thinking, reflection."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ken Arnold",
    "section": "",
    "text": "I’m interested in research and teaching at the intersection of data science, people, and Christian faith. My main projects are around AI for Everyday Creativity, including:"
  },
  {
    "objectID": "index.html#highlighted-posts",
    "href": "index.html#highlighted-posts",
    "title": "Ken Arnold",
    "section": "Highlighted Posts",
    "text": "Highlighted Posts\n\n\n\n\n\n\nTransparency for Generated Text\n\n\n\n\nLightweight marks for AI-generated text\n\n\n\n\n\n\nFeb 17, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nDoes increasing ‘alignment’ increase risk?\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nLanguage Models have Personas?\n\n\n\n\nRLHF is fascinating.\n\n\n\n\n\n\nFeb 6, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nHumanity of the Gaps\n\n\n\n\nReality is spiky.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nAvoid Magical Thinking about AI\n\n\n\n\nEven if you’re not a computer scientist, you should learn enough about AI that you can steer its progress.\n\n\n\n\n\n\nJan 24, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nStop trying to detect computer-generated writing.\n\n\n\n\nIt’s a risky distraction that just encourages an arms race.\n\n\n\n\n\n\nJan 17, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nWithout Acknowledgment: Cultivating Gratitude in an ML age\n\n\n\n\nCreativity support tools based on machine learning hide the human authors that contributed to the work. Does it have to be that way?\n\n\n\n\n\n\nJan 12, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nTwo Ways that LLMs Hallucinate\n\n\n\n\nDo you ever make your decision first and think about your reasons later?\n\n\n\n\n\n\nFeb 16, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nOn being unsurprised by the Internet\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\n0 min\n\n\n\n\n\n\n\n\nChatGPT Doesn’t Have a Database of Facts\n\n\n\n\nIt was trained to be believable, not to be right.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nSelf-Study Advice\n\n\n\n\nPractice solving problems.\n\n\n\n\n\n\nJan 30, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nMultidimensional Array Terminology\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nSolomon’s proverbs about AI\n\n\n\n\nA reflection without words on the difference between wisdom and its imitation.\n\n\n\n\n\n\nJan 16, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nLanguage Models for Species-Scale Collaboration\n\n\n\n\nHow knowledge fusion will continue the trend of scaling humam collaboration\n\n\n\n\n\n\nJan 11, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\nAll posts"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Ken Arnold",
    "section": "Teaching",
    "text": "Teaching\nI teach computer science, data science, and machine learning. Recent classes:\n\nDATA 202 Data Science 2, a data wrangling, predictive modeling, and visualization course using the R tidyverse, with a project emphasis\nCS 344 Artificial Intelligence, a hands-on (but also concept-heavy) machine learning course based on the fast.ai course and Hugging Face Transformers\nCS 108 Introduction to Computing and CS 106, Calvin’s first-year Computer Science course, in Python.\nINFO 602 Predictive Analytics (for Calvin’s MBA program)\n\nSee CV for others."
  },
  {
    "objectID": "index.html#doing-research-with-me",
    "href": "index.html#doing-research-with-me",
    "title": "Ken Arnold",
    "section": "Doing research with me",
    "text": "Doing research with me\nI welcome students who are interested in human-computer interaction, machine learning, data science, interactive visualization, and Christian perspectives on data and computation. I have a range of ideas and ongoing projects, but bring your own interests too! Please contact me via email (Calvin: ka37) or Teams."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ken Arnold",
    "section": "Education",
    "text": "Education\n\nPhD in Computer Science, Harvard, 2020\nMS in Media Arts and Sciences, MIT, 2010\nBS in Electrical and Computer Engineering, Cornell, 2007"
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Some selected publications are below. For a more complete list, see my Google Scholar profile. For less-polished but more recent thoughts, see the blog posts here.\n\nFaithful text prediction | Christian Courier\nChatGPT is not Magic (Talk)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Writing: Do we speak the words of stochastic parrots?\n\n\n\n\n\n\n\nHCI\n\n\nNLP\n\n\n\n\nMimicry pervades the design and implementation of generative AI systems. I reflect on this observation from a Christian perspective and outline various approaches to ‘redeem’ how we use the artificial parrots in writing.\n\n\n\n\n\n\nJun 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGenerative Models can Help Writers without Writing for Them\n\n\n\n\n\n\n\nHCI\n\n\nNLP\n\n\n\n\nCurrent writing-support systems encourage writers to appropriate the language model’s output as their own words. Does it have to be that way? We explore of two new types of interactions with generative language models: enabling structural manipulation of already-drafted sentences, and offering questions rather than snippets to writers. We present early experiments on feasibility and suitability of these interactions.\n\n\n\n\n\n\nApr 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPredictive Text Encourages Predictable Writing\n\n\n\n\n\n\n\nHCI\n\n\nNLP\n\n\n\n\nIn our lab study, image captions written with predictive text suggestions were shorter and included fewer words that that the system did not predict.\n\n\n\n\n\n\nMar 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Bias in Predictive Text Recommendations Results in Biased Writing\n\n\n\n\n\n\n\nHCI\n\n\nNLP\n\n\n\n\nIntelligent systems make biased decisions because they are trained on biased data. Could these system biases affect what people create? We found that when writing restaurant reviews, biased system behavior leads to biased human behavior: People presented with phrasal text entry shortcuts that were skewed positive wrote more positive reviews than they did when presented with negative-skewed shortcuts.\n\n\n\n\n\n\nMay 8, 2018\n\n\n\n\n\n\n  \n\n\n\n\nOn Suggesting Phrases vs. Predicting Words for Mobile Text Composition\n\n\n\n\n\n\n\nHCI\n\n\nNLP\n\n\n\n\nWe introduce a simple extension to the familiar mobile keyboard suggestion interface that presents phrase suggestions that can be accepted by a repeated-tap gesture. In an extended composition task, we found that phrases were interpreted as suggestions that affected the content of what participants wrote more than conventional single-word suggestions, which were interpreted as predictions.\n\n\n\n\n\n\nOct 16, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/new_years_2020/new_years_2020.html",
    "href": "static/new_years_2020/new_years_2020.html",
    "title": "Arnolds New Years Letter 2020",
    "section": "",
    "text": "Happy New Decade!\nHighlights:\n\nWe’ve moved to Grand Rapids, Michigan! We miss Boston and our friends there (who helped us immensely with the move, by the way), but we’ve started to appreciate Grand Rapids on its own terms.\nKen started teaching Computer Science and Data Science at Calvin University! The first semester was joyfully overwhelming. Ken feels increasingly thankful and blessed to be there.\nSusan has mostly been at home with Naomi and Esther. For ten weeks last spring, she went back to work part time for her former architecture firm, with the help of a wonderful nanny. She felt like that was a nice balance and is hoping to find a similar part time schedule here, once she finishes her architecture professional exams.\nNaomi and Esther celebrated their second birthday in November. They have learned so much, going from not talking at the beginning of the year, to putting ever more complex phrases together.\n\nOther tidbits:\n\n2019 started with a New Year’s Day hike in Maryland with Ken’s brother Austin and (now) fiancée Mason, and finished with Ken’s parents visiting us in Grand Rapids for New Years 2020.\nKen’s parents visited us frequently during the year (coming from Maryland) and Susan’s parents visited Boston over the summer to help us pack up for our move.\nNaomi and Esther were baptized at Citylife Church in Boston in June, with all grandparents attending. Baptism is a symbol of the faith we share and our hopes that the girls will grow in that faith as part of Christian community.\nWe enjoyed Christmas in Portland, Oregon with Susan’s parents.\nWe are now located closer to Susan’s extended family in Michigan.\nWe bought a minivan (a like-new Honda Odyssey)!\n\nLooking forward to:\n\nKen teaching another two classes this spring, and hopefully finally defending his PhD thesis (long story).\nUpcoming travels: to Ken’s brother Austin’s wedding mid-October, Ken traveling to Italy in March to present a paper at a conference (it finally got accepted on the third try), and maybe a few days vacationing by Lake Michigan this summer with one of Susan’s friends.\nGetting connected with a local church.\nSusan continuing in professional development and maybe starting work.\nNaomi and Esther continuing to grow, and we’re considering starting preschool this year.\n\nOur new address is 3654 Palmer Ridge Road SE, Grand Rapids MI 49546.\nWith love,\nKen, Susan, Esther and Naomi\n\n\n\nNaomi and Esther\n\n\nmore photos here."
  },
  {
    "objectID": "pubs/iui21-help-writers/index.html",
    "href": "pubs/iui21-help-writers/index.html",
    "title": "Generative Models can Help Writers without Writing for Them",
    "section": "",
    "text": "PDF HAI-GEN Workshop at IUI 2021"
  },
  {
    "objectID": "pubs/iui21-help-writers/index.html#abstract",
    "href": "pubs/iui21-help-writers/index.html#abstract",
    "title": "Generative Models can Help Writers without Writing for Them",
    "section": "Abstract",
    "text": "Abstract\nComputational models of language have the exciting potential to help writers generate and express their ideas. Current approaches typically provide their outputs to writers in a way that writers can (and often do) appropriate as their own—giving the system more control than necessary over the final outcome of the writing. We present early explorations of two new types of interactions with generative language models; both share the design goal of keeping the writer in ultimate control while providing generative assistance. One interaction enables new kinds of structural manipulation of already-drafted sentences; it keeps the writer in semantic control by conditioning the output to be a paraphrase of human-provided input. The other interaction enables new kinds of idea exploration by offering questions rather than snippets to writers; it keeps the writer in semantic control by providing its ideas in an open-ended form. We present the results of our early experiments on the feasibility and suitability of these types of interactions."
  },
  {
    "objectID": "pubs/sentiment-bias/index.html",
    "href": "pubs/sentiment-bias/index.html",
    "title": "Sentiment Bias in Predictive Text Recommendations Results in Biased Writing",
    "section": "",
    "text": "PDF In GI’18\nNote: The authors of this paper, published in CHI 2023, independently replicated our study but with better design, much improved language model, and much larger scale. The basic findings still hold, and the implications are just as concerning. Co-Writing with Opinionated Language Models Affects Users’ Views | Abstract"
  },
  {
    "objectID": "pubs/sentiment-bias/index.html#abstract",
    "href": "pubs/sentiment-bias/index.html#abstract",
    "title": "Sentiment Bias in Predictive Text Recommendations Results in Biased Writing",
    "section": "Abstract",
    "text": "Abstract\nPrior research has demonstrated that intelligent systems make biased decisions because they are trained on biased data. As people increasingly leverage intelligent systems to enhance their productivity and creativity, could system biases affect what people create? We demonstrate that in at least one domain (writing restaurant reviews), biased system behavior leads to biased human behavior: People presented with phrasal text entry shortcuts that were skewed positive wrote more positive reviews than they did when presented with negative-skewed shortcuts. This result contributes to the pertinent debate about the role of intelligent systems in our society."
  },
  {
    "objectID": "pubs/predictable/index.html",
    "href": "pubs/predictable/index.html",
    "title": "Predictive Text Encourages Predictable Writing",
    "section": "",
    "text": "DOI PDF IUI 2020"
  },
  {
    "objectID": "pubs/predictable/index.html#abstract",
    "href": "pubs/predictable/index.html#abstract",
    "title": "Predictive Text Encourages Predictable Writing",
    "section": "Abstract",
    "text": "Abstract\nIntelligent text entry systems, including the now-ubiquitous predictive keyboard, can make text entry more efficient, but little is known about how these systems affect the content that people write. To study how predictive text systems affect content, we compared image captions written with different kinds of predictive text suggestions. Our key findings were that captions written with suggestions were shorter and that they included fewer words that that the system did not predict. Suggestions also boosted text entry speed, but with diminishing benefit for faster typists. These findings imply that text entry systems should be evaluated not just by speed and accuracy but also by their effect on the content written."
  },
  {
    "objectID": "pubs/phrase-suggestion/index.html",
    "href": "pubs/phrase-suggestion/index.html",
    "title": "On Suggesting Phrases vs. Predicting Words for Mobile Text Composition",
    "section": "",
    "text": "PDF In UIST’16"
  },
  {
    "objectID": "pubs/phrase-suggestion/index.html#abstract",
    "href": "pubs/phrase-suggestion/index.html#abstract",
    "title": "On Suggesting Phrases vs. Predicting Words for Mobile Text Composition",
    "section": "Abstract",
    "text": "Abstract\nA system capable of suggesting multi-word phrases while someone is writing could supply ideas about content and phrasing and allow those ideas to be inserted efficiently. Meanwhile, statistical language modeling has provided various approaches to predicting phrases that users type. We introduce a simple extension to the familiar mobile keyboard suggestion interface that presents phrase suggestions that can be accepted by a repeated-tap gesture. In an extended composition task, we found that phrases were interpreted as suggestions that affected the content of what participants wrote more than conventional single-word suggestions, which were interpreted as predictions. We highlight a design challenge: how can a phrase suggestion system make valuable suggestions rather than just accurate predictions?"
  },
  {
    "objectID": "pubs/acms-mimicry/index.html",
    "href": "pubs/acms-mimicry/index.html",
    "title": "AI and Writing: Do we speak the words of stochastic parrots?",
    "section": "",
    "text": "PDF ACMS 2022"
  },
  {
    "objectID": "pubs/acms-mimicry/index.html#abstract",
    "href": "pubs/acms-mimicry/index.html#abstract",
    "title": "AI and Writing: Do we speak the words of stochastic parrots?",
    "section": "Abstract",
    "text": "Abstract\nGenerative Artificial Intelligence systems have recently and publicly become increasingly comparable to the capabilities of many humans in some domains, such as text and image generation. These systems are rapidly being incorporated into human contexts, for example as phone keyboards and email apps offer us AI-generated suggestions of words and even complete phrases to enter with a single tap or swipe. But these suggestions don’t just reduce physical effort; they reduce cognitive effort: we can accept a suggested phrase before even thinking about what to say. In this article, I first describe how mimicry pervades the design and implementation of generative AI systems and expound on some of the implications thereof. I then critique how these systems are designed and implemented in light of our God-given identity, a biblical perspective on mimicry, and a reflection on virtue. I finally outline several ``swords-to-plowshares’’ approaches that re-form generative AI technologies to support human thinking in writing tasks: structural editing tools, writer-directed questions to support writers without speaking for them, and visualizations to enable writers to reflect on their work."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Transparency for Generated Text\n\n\n\n\n\n\n\nml\n\n\nllm\n\n\nnlp\n\n\n\n\nLightweight marks for AI-generated text\n\n\n\n\n\n\nFeb 17, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Ways that LLMs Hallucinate\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\n\n\nDo you ever make your decision first and think about your reasons later?\n\n\n\n\n\n\nFeb 16, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn being unsurprised by the Internet\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes increasing ‘alignment’ increase risk?\n\n\n\n\n\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models have Personas?\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\n\n\nRLHF is fascinating.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHumanity of the Gaps\n\n\n\n\n\n\n\nai\n\n\nperspectival\n\n\n\n\nReality is spiky.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT Doesn’t Have a Database of Facts\n\n\n\n\n\n\n\nai\n\n\nml\n\n\n\n\nIt was trained to be believable, not to be right.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Study Advice\n\n\n\n\n\n\n\ncoding\n\n\n\n\nPractice solving problems.\n\n\n\n\n\n\nJan 30, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultidimensional Array Terminology\n\n\n\n\n\n\n\nml\n\n\ncoding\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid Magical Thinking about AI\n\n\n\n\n\n\n\nai\n\n\nml\n\n\n\n\nEven if you’re not a computer scientist, you should learn enough about AI that you can steer its progress.\n\n\n\n\n\n\nJan 24, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStop trying to detect computer-generated writing.\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\n\n\nIt’s a risky distraction that just encourages an arms race.\n\n\n\n\n\n\nJan 17, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolomon’s proverbs about AI\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\n\n\nA reflection without words on the difference between wisdom and its imitation.\n\n\n\n\n\n\nJan 16, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithout Acknowledgment: Cultivating Gratitude in an ML age\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\nia\n\n\n\n\nCreativity support tools based on machine learning hide the human authors that contributed to the work. Does it have to be that way?\n\n\n\n\n\n\nJan 12, 2023\n\n\nKen Arnold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models for Species-Scale Collaboration\n\n\n\n\n\n\n\nai\n\n\nml\n\n\nlm\n\n\nia\n\n\n\n\nHow knowledge fusion will continue the trend of scaling humam collaboration\n\n\n\n\n\n\nJan 11, 2023\n\n\nKen Arnold\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Bios",
    "section": "",
    "text": "Assistant professor of computer science and data science at Calvin University. Crafting tools to augment intelligence, teaching students to build and use tech wisely."
  },
  {
    "objectID": "bio.html#medium50-words",
    "href": "bio.html#medium50-words",
    "title": "Bios",
    "section": "Medium(50 words)",
    "text": "Medium(50 words)\nKen Arnold (B.S., Cornell; M.S., MIT; Ph.D., Harvard) is an assistant professor of computer science and data science at Calvin University. His research has shown how predictive text interfaces, like those in smartphone keyboards and email apps, can shape the content of what people communicate. His current research interests include human-AI interaction in communication, creativity, and education."
  },
  {
    "objectID": "bio.html#longer",
    "href": "bio.html#longer",
    "title": "Bios",
    "section": "Longer",
    "text": "Longer\nKen Arnold (B.S., Cornell; M.S., MIT; Ph.D., Harvard) is an assistant professor of computer science and data science at Calvin University. His research has shown how predictive text interfaces, like those in smartphone keyboards and email apps, can shape the content of what people communicate. He is currently working on intelligence augmentation to help writers craft words that are fully their own. His current research interests include human-AI interaction in communication, creativity, and education."
  }
]